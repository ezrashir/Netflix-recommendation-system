{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netflix personal reccomandation system - Semi-supervised learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the amount of Netflix samples (~9000) compared to the amount of my labeled Netflix data (~200), the difference is too big to allow a supervised classification. Moreover, when considering all the features (including the ones that require encoding), the number of features in a supervised algo will be very large relative to the number of available labeled samples (will probably result in sparse data and overfitting). Thus a different approach is required. Here, I will create a semi-supervised algorithm that uses the abundant Netflix unlabeled damples in order to create features that will feed a supervised classification trained on the labeled data. \n",
    "For the unsupervised part I thought of using a Association rule mining algo but since I have several continuous features (binning was too crude and will also result in sparse data), I decided to take a different approach.\n",
    "I will use a k-means algo for the unsupervised learning so feature scalling is required (distances based algorithm). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec algo:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: The next lesson will be to switch the Word2Vec algo with the Bert algo.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we will create a Word2Vec class in order to use the Title and most importantly- Description features. The reason for creating the class is to use different Word2Vec models for the different features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "class Word2Vec_feature_creation():\n",
    "    def __init__(self, vector_size, train_data, data2embed, window=1):\n",
    "        self.vec_size = vector_size\n",
    "        self.training_data = train_data\n",
    "        self.window = window\n",
    "        self.data2embed = data2embed\n",
    "    \n",
    "    def tokenize(self, data):\n",
    "        tokenized_data = [gensim.utils.simple_preprocess(text) for text in data]\n",
    "        return tokenized_data\n",
    "\n",
    "    def train(self):\n",
    "        tokenized_train = self.tokenize(self.training_data)\n",
    "        return Word2Vec(tokenized_train, vector_size=self.vec_size, window=self.window, sample=1e-5, min_count=1, workers=1, seed=2)\n",
    "\n",
    "    def word_embedding(self):\n",
    "        embedded = []\n",
    "        model = self.train()\n",
    "        tokenized_data = self.tokenize(self.data2embed)\n",
    "        for sentence in tokenized_data:\n",
    "            embeddings = [model.wv[word] for word in sentence]\n",
    "            if embeddings:\n",
    "                sentence_embedding = sum(embeddings) / len(embeddings)  # Average of word embeddings\n",
    "                embedded.append(sentence_embedding)\n",
    "        return embedded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we want to start visualizing the different features and apply the Word2Vec word embedding on the relevant features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General view:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the director has a significant amount of empty values and the values cannot be taken as the mean value, we will skip this feature since using it can create a strong bias in the network. \n",
    "Let’s dive deeper into the remaining feature:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_N = pd.read_csv(r\"INSER FILE LOCATION HERE\", skipinitialspace = True, skip_blank_lines=True)\n",
    "print(df_N.isna().sum())\n",
    "df_N.drop(['show_id', 'country', 'date_added', 'director'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 samples with NaN value in the rating column. Since it’s such a small amount relative to the number of samples, we will choose the most frequent option to fill the 4 blanks. Let's see the categories and their frequancy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_data = df_N['rating'].dropna().astype(str)\n",
    "unique_rating = np.unique(rating_data.dropna())\n",
    "\n",
    "x, _, _ = plt.hist(rating_data, bins=np.size(unique_rating), range=(-0.5,13.5), histtype='step')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('frequancy')\n",
    "y_line = 6\n",
    "plt.hlines(y=y_line, xmin=-0.5, xmax=14, color='red', linestyle='--')\n",
    "plt.text(x=12, y=y_line+1, s='Frequency = ' + str(y_line), fontsize=8, color='red')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since the most frequant value is 'TV-MA', we can assign this values to all NaN vales in the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_N['rating'].fillna('TV-MA', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking a bit closer will show us that there are 3 categories (‘TV-Y7-FV’, ‘UR’ and ‘NC-17’) with less than 7 occurrences. Since the rating is a categorical feature, creating three more columns (one for each category below the red line) will result in a very sparse network and will add very little information to the network. Thus, we will eliminate the samples related to these three categories. We will now remove the relevant samples from the Netflix dataset (df_N). Since we know we have 12 samples to remove, printing the shape of the 0 axis of the dataset will insure we removed the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_inx = df_N[(df_N['rating'] == 'TV-Y7-FV')  | (df_N['rating'] == 'UR') | (df_N['rating'] == 'NC-17')].index\n",
    "print(df_N.shape[0])\n",
    "df_N.drop(drop_inx, inplace=True)\n",
    "df_N.reset_index(inplace=True, drop=True)\n",
    "print(df_N.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the rating is a single column, we can use sklearn.preproccessijg.OneHotEncoder to encode the categories. We will create a 'features' df to store all the processed features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an encoder for the 'Rating' feature\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_columns = encoder.fit_transform(df_N['rating'].to_frame())\n",
    "feature_values = encoder.categories_[0]\n",
    "column_names = [value for value in feature_values]\n",
    "features = pd.DataFrame(encoded_columns, columns=column_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories (Listed in):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The listed_in column contained multiple values for each sample (up to 3). The first step was to seperate them to 3 different columns. Next step was to examine the data. Since the first column has no NaN values, each sample is associated with at least 1 and up to 3 categories. Let's plot a histogram of all the possible values (without NaNs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listedIn_dataframe = df_N.loc[:,'listed_in1':'listed_in3'].fillna('')\n",
    "listedIn_flat = listedIn_dataframe.to_numpy().flatten()\n",
    "listedIn_flat = np.delete(listedIn_flat, np.where(listedIn_flat == ''))\n",
    "unique_LI= np.unique(listedIn_flat)\n",
    "\n",
    "hist = plt.hist(listedIn_flat, bins=np.size(unique_LI), histtype='step')\n",
    "plt.xlabel('Listed in')\n",
    "plt.ylabel('frequancy')\n",
    "y_line=50\n",
    "plt.hlines(y=y_line, xmin=0, xmax=44, color='red', linestyles='--', linewidth=0.7)\n",
    "plt.text(x=38, y=y_line+3, s='Frequency = ' + str(y_line), fontsize=8, color='red')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the features for the 'Listed In' features. The processed features will be save in a df named 'features'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategorizeListedIn(dataframe):\n",
    "    samplesNum = np.size(dataframe, 0)\n",
    "    unique_vals = np.unique(dataframe.to_numpy().flatten())[1:]\n",
    "    features = pd.DataFrame(columns = unique_vals, index = np.arange(0, samplesNum)).fillna(0)\n",
    "    for i in range(samplesNum):\n",
    "        feature0 = dataframe['listed_in1'][i]\n",
    "        features[feature0][i] = 1\n",
    "        c = dataframe['listed_in3'][49]\n",
    "        if dataframe['listed_in2'][i] != '':\n",
    "            feature1 = dataframe['listed_in2'][i]\n",
    "            features[feature1][i] = 1\n",
    "        if dataframe['listed_in3'][i] != '':\n",
    "            feature2 = dataframe['listed_in3'][i]\n",
    "            features[feature2][i] = 1\n",
    "    return features\n",
    "\n",
    "y = df_N.loc[:,'listed_in1':'listed_in3'].fillna('')\n",
    "listedIn_features = CategorizeListedIn(df_N.loc[:,'listed_in1':'listed_in3'].fillna(''))\n",
    "features = pd.concat([features, listedIn_features], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the 'Type' has 0 Nan value, and it contains twi categories: 'Movie' and 'TV Show', we can go ahead and create the categorical features for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an encoder for the 'Type' feature\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_columns = encoder.fit_transform(df_N['Type'].to_frame())\n",
    "feature_values = encoder.categories_[0]\n",
    "column_names = ['Type_' + value for value in feature_values]\n",
    "encoded_df = pd.DataFrame(encoded_columns, columns=column_names)\n",
    "features = pd.concat([features, encoded_df], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release Year:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, all samples have the release year listed. Let's check the histogram of the category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_data = df_N['release_year']\n",
    "unique_release = np.unique(release_data)\n",
    "\n",
    "plt.hist(release_data.sort_values(), bins=np.arange(release_data.min() + 1.5, release_data.max() + 1.5) - 0.5, histtype='step')\n",
    "plt.xlabel('Release year')\n",
    "plt.ylabel('frequancy')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the frequancy of the release year increases exponentialy as we are getting closer to the present. This feature needs only scaling in the preprocessing stage. Since the data is exponential we want to consider renormalizing it. The reason we might want to renormalize it is because the shape of the distribution was determined by the development of technology and the increasing popularty of Netflix and not due to some feature of each specific content. Nevertheless, the features of the content is directly related to it's release year and this can directly influence the popularity of the show. Here, in order to take both considerations under account, we will use a Quantile Transformer Scaler that is not sensitive to outliers but also transforms the features to a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "QSc = QuantileTransformer(output_distribution='uniform')\n",
    "feature_values = QSc.fit_transform(release_data.to_frame())\n",
    "features['release_year'] = feature_values.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Duration' feature have zero missing values. A quick check can teel us that for each one of the samples of type 'Movie' the duration is written in 'min' and for samples of type 'TV Show' the duration is the number of seasons: \n",
    "(Run if you want to double check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds = 0\n",
    "for i in range(df_N.shape[0]):\n",
    "    if df_N['Type'][i] == 'TV Show' and 'min' in df_N['duration'][i]:\n",
    "        odds += 1\n",
    "    if df_N['Type'][i] == 'Movie' and 'Season' in df_N['duration'][i]:\n",
    "        odds += 1\n",
    "print('number of odds is: ' + str(odds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the difference between the 'min' and 'Seasons' is already accounted for by the 'Type' feature, all we need to do here is change the data to int instead of strings and later on apply an appropriate scaler. To split the strings to seperate columns we will use the pandas split function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_feature = df_N['duration'].str.split(expand=True)[:][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the distributions of the data seperatly for movies and TV shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_duration = duration_feature[df_N[df_N['Type'] == 'Movie'].index].astype(int)\n",
    "TvShow_duration = duration_feature[df_N[df_N['Type'] == 'TV Show'].index].astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "fig.subplots_adjust(hspace=0.348)\n",
    "ax[0].hist(movies_duration.sort_values(), histtype='step', bins=np.arange(0, movies_duration.max() + 1.5) - 0.5)\n",
    "ax[0].set_ylabel('frequancy')\n",
    "ax[1].set_xlabel('Minutes')    \n",
    "ax[0].set_title('Movies')\n",
    "ax[1].hist(TvShow_duration.sort_values(), histtype='step', bins=np.arange(0, TvShow_duration.max() + 1.5) - 0.5)\n",
    "ax[1].set_ylabel('frequancy') \n",
    "ax[1].set_xlabel('Seasons')    \n",
    "ax[1].set_xticks(range(0, 18, 1))\n",
    "ax[1].set_title('TV Shows')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split the feature into two columns, the duration of movies and the duration of TV shows since there is a difference in the units of measurement for each \"Type\" of sample (the duration of movies is counted in minuts and the duration of tv shows in seasons). \n",
    "The movies disrtibution is similar to a normal distribution with mean of ~100 only it's a bit skewed on the left side. This feature will need to be scaled using standard scaler which fits for the scaling of normally distributed data. \n",
    "\n",
    "The Seasons number distributes exponentially. This time we can assume the exponential decrease of the number of seasons is strongly related to the successs of the Tv Show. A min-max scaler is better suited for this distribution. Nevertheless, using these two different scalers for the two duration features will result in a bigger weight of the 'TV show duration' since the min-max-scaling has a stronger shrinking effect than standard scaling.\n",
    "\n",
    "We will thus use a standard scaler for both features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_N' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m duration_feature \u001b[39m=\u001b[39m df_N[\u001b[39m'\u001b[39m\u001b[39mduration\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39msplit(expand\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[:][\u001b[39m0\u001b[39m]\n\u001b[0;32m      2\u001b[0m features_vals \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\u001b[39m0\u001b[39m, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mduration_movies\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mduration_TvShows\u001b[39m\u001b[39m'\u001b[39m], index\u001b[39m=\u001b[39m\u001b[39mrange\u001b[39m(df_N\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(df_N\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_N' is not defined"
     ]
    }
   ],
   "source": [
    "duration_feature = df_N['duration'].str.split(expand=True)[:][0]\n",
    "features_vals = pd.DataFrame(0, columns=['duration_movies', 'duration_TvShows'], index=range(df_N.shape[0]))\n",
    "for i in range(df_N.shape[0]):\n",
    "    val = df_N['duration'][i].split()[0]\n",
    "    if 'min' in df_N['duration'][i]:\n",
    "        features_vals['duration_movies'][i] = val\n",
    "    else:\n",
    "        features_vals['duration_TvShows'][i] = val\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SSc_movies = StandardScaler()\n",
    "features_vals['duration_movies'] = SSc_movies.fit_transform(features_vals['duration_movies'].to_frame())\n",
    "SSc_series = StandardScaler()\n",
    "features_vals['duration_TvShows'] = SSc_series.fit_transform(features_vals['duration_TvShows'].to_frame())\n",
    "\n",
    "features = pd.concat([features, features_vals], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Title & Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the Title and description, we will use the Word2Vec class previously created for extracting features from the string parameters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuralNetwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
