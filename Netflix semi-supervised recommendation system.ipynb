{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Netflix personal reccomandation system - Semi-supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do:\n",
    "1) move the section of the feature with the sampples deletion to be first since it's a shame.\n",
    "2) cahnge the order of the dataset and create a sorted features database that has all the features. \n",
    "3) check if it's cool that the initial word2vec algo is training on the description.\n",
    "4) copy-paste all the code sections after finishing working in the completerunningcode.py file.\n",
    "\n",
    "TO DO BEFORE I go back to that:\n",
    "1) in categories delete the \" Movies\" and \" TV Shows\" ensings from the values because it's also exactly like the \"type\" feature AND we also removed it in the ratings section so we need to be consistent. ALSO remove the \"TV Shows\" category, it's absured. And basically look at the categories because there are \"TV Sci-Fi & Fantasy\" AND \"Sci-Fi & Fantasy\" category. \n",
    "2) consider in the \"caegory\" feature to create a single column with different sets of scaled data. meaning the movies will be scales by movies and tv shows scaled by tv shows but all the data appears in one column. What do I have to say about that? think deeply if it makes sence or not and WHY."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the amount of Netflix samples (~9000) compared to the amount of my labeled Netflix data (~200), the difference is too big to allow a supervised classification. Moreover, when considering all the features (including the ones that require encoding), the number of features in a supervised algo will be very large relative to the number of available labeled samples (will probably result in sparse data and overfitting). Thus a different approach is required. Here, I will create a semi-supervised algorithm that uses the abundant Netflix unlabeled samples in order to create features that will feed a supervised classification trained on the labeled data. \n",
    "For the unsupervised part I thought of using a Association rule mining algo but since I have several continuous features (binning was too crude and will also result in sparse data), I decided to take a different approach.\n",
    "I will use a k-means algo for the unsupervised learning so feature scalling is required (distances based algorithm). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Preprocessing:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Numeric and categorical features:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the director has a significant amount of empty values and the values cannot be taken as the mean value, we will skip this feature since using it can create a strong bias in the network. \n",
    "Let’s dive deeper into the remaining feature:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_N = pd.read_csv(r\"INSER FILE LOCATION HERE\", skipinitialspace = True, skip_blank_lines=True)\n",
    "print(df_N.isna().sum())\n",
    "df_N.drop(['show_id', 'country', 'date_added', 'director'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1) Rating:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will merge 'G'/'PG' and 'TV-G'/'TV-PG' ratings as their difference is in the \"Type\" property (which is a seperate feature). \n",
    "Second, There are 4 samples with NaN value in the rating column. Since it’s such a small amount relative to the number of samples, we will choose the most frequent option to fill the 4 blanks. Let's see the categories and their frequancy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_N' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Shir ASUS laptop\\New projects\\NeuralNetwork\\Netflix semi-supervised recommendation system.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Shir%20ASUS%20laptop/New%20projects/NeuralNetwork/Netflix%20semi-supervised%20recommendation%20system.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_N[\u001b[39m'\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df_N[\u001b[39m'\u001b[39m\u001b[39mratings\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mTV-G\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mG\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Shir%20ASUS%20laptop/New%20projects/NeuralNetwork/Netflix%20semi-supervised%20recommendation%20system.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m rating_data \u001b[39m=\u001b[39m df_N[\u001b[39m'\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdropna()\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Shir%20ASUS%20laptop/New%20projects/NeuralNetwork/Netflix%20semi-supervised%20recommendation%20system.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m unique_rating \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(rating_data\u001b[39m.\u001b[39mdropna())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_N' is not defined"
     ]
    }
   ],
   "source": [
    "df_N['rating'] = df_N['rating'].replace('TV-G', 'G')\n",
    "df_N['rating'] = df_N['rating'].replace('TV-PG', 'PG')\n",
    "rating_data = df_N['rating'].dropna().astype(str)\n",
    "unique_rating = np.unique(rating_data.dropna())\n",
    "\n",
    "x, _, _ = plt.hist(rating_data, bins=np.size(unique_rating), range=(-0.5,13.5), histtype='step')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('frequancy')\n",
    "y_line = 6\n",
    "plt.hlines(y=y_line, xmin=-0.5, xmax=14, color='red', linestyle='--')\n",
    "plt.text(x=12, y=y_line+1, s='Frequency = ' + str(y_line), fontsize=8, color='red')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, since the most frequant value is 'TV-MA', we can assign this values to all NaN vales in the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_N['rating'].fillna('TV-MA', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking a bit closer will show us that there are 3 categories (‘TV-Y7-FV’, ‘UR’ and ‘NC-17’) with less than 7 occurrences. Since the rating is a categorical feature, creating three more columns (one for each category below the red line) will result in a very sparse network and will add very little information to the network. Thus, we will eliminate the samples related to these three categories. We will now remove the relevant samples from the Netflix dataset (df_N). Since we know we have 12 samples to remove, printing the shape of the 0 axis of the dataset will insure we removed the rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_inx = df_N[(df_N['rating'] == 'TV-Y7-FV')  | (df_N['rating'] == 'UR') | (df_N['rating'] == 'NC-17')].index\n",
    "print(df_N.shape[0])\n",
    "df_N.drop(drop_inx, inplace=True)\n",
    "df_N.reset_index(inplace=True, drop=True)\n",
    "print(df_N.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the rating is a single column, we can use sklearn.preproccessijg.OneHotEncoder to encode the categories. We will create a 'features' df to store all the processed features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an encoder for the 'Rating' feature\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_columns = encoder.fit_transform(df_N['rating'].to_frame())\n",
    "feature_values = encoder.categories_[0]\n",
    "column_names = [value for value in feature_values]\n",
    "features = pd.DataFrame(encoded_columns, columns=column_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2) Categories (Listed in):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The listed_in column contained multiple values for each sample (up to 3). The first step was to seperate them to 3 different columns. Next step was to examine the data. Since the first column has no NaN values, each sample is associated with at least 1 and up to 3 categories. Let's plot a histogram of all the possible values (without NaNs). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listedIn_dataframe = df_N.loc[:,'listed_in1':'listed_in3'].fillna('')\n",
    "listedIn_flat = listedIn_dataframe.to_numpy().flatten()\n",
    "listedIn_flat = np.delete(listedIn_flat, np.where(listedIn_flat == ''))\n",
    "unique_LI= np.unique(listedIn_flat)\n",
    "\n",
    "hist = plt.hist(listedIn_flat, bins=np.size(unique_LI), histtype='step')\n",
    "plt.xlabel('Listed in')\n",
    "plt.ylabel('frequancy')\n",
    "y_line=50\n",
    "plt.hlines(y=y_line, xmin=0, xmax=44, color='red', linestyles='--', linewidth=0.7)\n",
    "plt.text(x=38, y=y_line+3, s='Frequency = ' + str(y_line), fontsize=8, color='red')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the features for the 'Listed In' features. The processed features will be save in a df named 'features'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategorizeListedIn(dataframe):\n",
    "    samplesNum = np.size(dataframe, 0)\n",
    "    unique_vals = np.unique(dataframe.to_numpy().flatten())[1:]\n",
    "    features = pd.DataFrame(columns = unique_vals, index = np.arange(0, samplesNum)).fillna(0)\n",
    "    for i in range(samplesNum):\n",
    "        feature0 = dataframe['listed_in1'][i]\n",
    "        features[feature0][i] = 1\n",
    "        if dataframe['listed_in2'][i] != '':\n",
    "            feature1 = dataframe['listed_in2'][i]\n",
    "            features[feature1][i] = 1\n",
    "        if dataframe['listed_in3'][i] != '':\n",
    "            feature2 = dataframe['listed_in3'][i]\n",
    "            features[feature2][i] = 1\n",
    "    return features\n",
    "\n",
    "y = df_N.loc[:,'listed_in1':'listed_in3'].fillna('')\n",
    "listedIn_features = CategorizeListedIn(df_N.loc[:,'listed_in1':'listed_in3'].fillna(''))\n",
    "features = pd.concat([features, listedIn_features], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3) Type:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the 'Type' has 0 Nan value, and it contains two categories: 'Movie' and 'TV Show', we can go ahead and create the categorical features for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an encoder for the 'Type' feature\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_columns = encoder.fit_transform(df_N['Type'].to_frame())\n",
    "feature_values = encoder.categories_[0]\n",
    "column_names = ['Type_' + value for value in feature_values]\n",
    "encoded_df = pd.DataFrame(encoded_columns, columns=column_names)\n",
    "features = pd.concat([features, encoded_df], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4) Release Year"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, all samples have the release year listed. Let's check the histogram of the category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_data = df_N['release_year']\n",
    "unique_release = np.unique(release_data)\n",
    "\n",
    "plt.hist(release_data.sort_values(), bins=np.arange(release_data.min() + 1.5, release_data.max() + 1.5) - 0.5, histtype='step')\n",
    "plt.xlabel('Release year')\n",
    "plt.ylabel('frequancy')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the frequancy of the release year increases exponentialy as we are getting closer to the present. This feature needs only scaling in the preprocessing stage. Since the data is exponential we want to consider renormalizing it. The reason we might want to renormalize it is because the shape of the distribution was determined by the development of technology and the increasing popularty of Netflix and not due to some feature of each specific content. Nevertheless, the features of the content is directly related to it's release year and this can directly influence the popularity of the show. Here, in order to take both considerations under account, we will use a Quantile Transformer Scaler that is not sensitive to outliers but also transforms the features to a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "QSc = QuantileTransformer(output_distribution='uniform')\n",
    "feature_values = QSc.fit_transform(release_data.to_frame())\n",
    "features['release_year'] = feature_values.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5) Duration:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Duration' feature have zero missing values. A quick check can tell us that for each one of the samples of type 'Movie' the duration is written in 'min' and for samples of type 'TV Show' the duration is the number of seasons.\n",
    "Let's make sure there aren't any exceptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds = 0\n",
    "for i in range(df_N.shape[0]):\n",
    "    if df_N['Type'][i] == 'TV Show' and 'min' in df_N['duration'][i]:\n",
    "        odds += 1\n",
    "    if df_N['Type'][i] == 'Movie' and 'Season' in df_N['duration'][i]:\n",
    "        odds += 1\n",
    "print('number of odds is: ' + str(odds))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the difference between the 'min' and 'Seasons' is already accounted for by the 'Type' feature, all we need to do here is change the data to int instead of strings and later on apply an appropriate scaler. To split the strings to seperate columns we will use the pandas split function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_feature = df_N['duration'].str.split(expand=True)[:][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the distributions of the data seperatly for movies and TV shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_duration = duration_feature[df_N[df_N['Type'] == 'Movie'].index].astype(int)\n",
    "TvShow_duration = duration_feature[df_N[df_N['Type'] == 'TV Show'].index].astype(int)\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "fig.subplots_adjust(hspace=0.348)\n",
    "ax[0].hist(movies_duration.sort_values(), histtype='step', bins=np.arange(0, movies_duration.max() + 1.5) - 0.5)\n",
    "ax[0].set_ylabel('frequancy')\n",
    "ax[1].set_xlabel('Minutes')    \n",
    "ax[0].set_title('Movies')\n",
    "ax[1].hist(TvShow_duration.sort_values(), histtype='step', bins=np.arange(0, TvShow_duration.max() + 1.5) - 0.5)\n",
    "ax[1].set_ylabel('frequancy') \n",
    "ax[1].set_xlabel('Seasons')    \n",
    "ax[1].set_xticks(range(0, 18, 1))\n",
    "ax[1].set_title('TV Shows')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split the feature into two columns, the duration of movies and the duration of TV shows since there is a difference in the units of measurement for each \"Type\" of sample (the duration of movies is counted in minuts and the duration of tv shows in seasons). \n",
    "The movies disrtibution is similar to a normal distribution with mean of ~100 only it's a bit skewed on the left side. This feature will need to be scaled using standard scaler which fits for the scaling of normally distributed data. \n",
    "\n",
    "The Seasons number distributes exponentially. This time we can assume the exponential decrease of the number of seasons is strongly related to the successs of the Tv Show. A min-max scaler is better suited for this distribution. Nevertheless, using these two different scalers for the two duration features will result in a bigger weight of the 'TV show duration' since the min-max-scaling has a stronger shrinking effect than standard scaling.\n",
    "\n",
    "We will thus use a standard scaler for both features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_feature = df_N['duration'].str.split(expand=True)[:][0]\n",
    "features_vals = pd.DataFrame(0, columns=['duration_movies', 'duration_TvShows'], index=range(df_N.shape[0]))\n",
    "for i in range(df_N.shape[0]):\n",
    "    val = df_N['duration'][i].split()[0]\n",
    "    if 'min' in df_N['duration'][i]:\n",
    "        features_vals['duration_movies'][i] = val\n",
    "    else:\n",
    "        features_vals['duration_TvShows'][i] = val\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "SSc_movies = StandardScaler()\n",
    "features_vals['duration_movies'] = SSc_movies.fit_transform(features_vals['duration_movies'].to_frame())\n",
    "SSc_series = StandardScaler()\n",
    "features_vals['duration_TvShows'] = SSc_series.fit_transform(features_vals['duration_TvShows'].to_frame())\n",
    "\n",
    "features = pd.concat([features, features_vals], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2) Text features: Title & Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the Title and description, we will use a pretrained Word2Vec class from gensim. (Note: The next lesson will be to try using the Bert algo instead.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec algo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the models we can use, a basic examination can be done using a standard Bag of words algorithm like Word2Vec. Keeping in mind that the average number of words in the content titles is roughly 2-6 words and the number of words in the description is about 60, we can use the same pretrained model for both features. Though the number of words in the two features differ in an order of magnitude, the number of words that actually contributes to the model (if not regarding words like: I, am, the, etc.) is roughly the same size. Thus, using the same model for vectorizing the two features would be sufficient. \n",
    "\n",
    "To examine the features, we begin with loading a pretrained model from gensim library and fetch it's vocabulary. Next step is writing a function that will generate a vocabulary from a dataframe and use it to create a vocabulary for the two features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import gensim.models\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-25')\n",
    "model_vocabulary = list(glove_vectors.index_to_key)\n",
    "\n",
    "def createVocab(df):\n",
    "    sentences = df.copy()          \n",
    "    vocab = []\n",
    "    tokenized = []\n",
    "    for i in range(len(sentences)):\n",
    "        if type(sentences[i]) != str:\n",
    "            sentences[i] = str(sentences[i])\n",
    "        temp = gensim.utils.simple_preprocess(sentences[i])\n",
    "        tokenized.append(temp)\n",
    "        for item in temp:\n",
    "            if item not in vocab:\n",
    "                vocab.append(item)\n",
    "    return vocab, tokenized\n",
    "title_vocab, title_tokenized = createVocab(df_N['Title'])\n",
    "description_vocab, description_tokenized = createVocab(df_N['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To asses the strategy for dealing with OOV (out of vocabulary) words, we need to consider the percentage of words in our vocabulary that does not exist in the model's vocabulary. Since we only have ~100 labeled samples for the suervised classification, even 0.5% of the words will be significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countMissing(vocab):\n",
    "    missing = []\n",
    "    for k in range(len(vocab)):\n",
    "        if vocab[k] not in model_vocabulary:\n",
    "            missing.append(vocab[k])\n",
    "    return missing       \n",
    "\n",
    "print('The percentage of words in \\'Title\\' not found in the model\\'s vocabulary: ', len(countMissing(title_vocab))/len(model_vocabulary))\n",
    "print('The percentage of words in \\'Description\\' not found in the model\\'s vocabulary: ', len(countMissing(description_vocab))/len(model_vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The percentage of words in 'Title' not found in the model's vocabulary:  0.0009166210031889027\n",
    "The percentage of words in 'Description' not found in the model's vocabulary:  0.0012961724789152032"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As both results are <0.05%, we can safely eliminate the words not present in the pretrained model's vocabulary. Another option will be to contniue training the model on the netflix features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for now, we will not filter out stop words in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_features(preprocessed_sentences):\n",
    "    sentence_vectors = []\n",
    "    for sentence in preprocessed_sentences:\n",
    "        words_in_vocab = [word for word in sentence if word in model_vocabulary]\n",
    "        if len(words_in_vocab) == 0:\n",
    "            continue\n",
    "\n",
    "        sentence_vector = glove_vectors[words_in_vocab].mean(axis=0)\n",
    "        sentence_vectors.append(sentence_vector)\n",
    "    return sentence_vectors\n",
    "\n",
    "title_features = create_text_features(title_tokenized)\n",
    "description_features = create_text_features(description_tokenized)\n",
    "\n",
    "features = pd.concat([features, title_features], axis=1)\n",
    "features = pd.concat([features, description_features], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is it for feature creation and preprocessing. \n",
    "Now, all that is left is to create the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Label prediction for unlabeled features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuralNetwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
